# 0 安装配置Spark

## 0.1 官网下载并解压

用虚拟机登录官网https://archive.apache.org/dist/spark/下载spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz

```bash
# 进入解压包存放的Downloads文件夹,然后解压压缩包至/home/chenmiao
sudo tar -zxf spark-3.5.3-bin-hadoop3.tgz -C /home/chenmiao
# 进入刚刚解压后存放的目录下，将该文件夹的名字重命名为spark
sudo mv ./spark-3.5.3-bin-hadoop3/ ./spark
```

## 0.2 安装依赖

安装 Spark 的 Python 包：

```
pip install pyspark
```

<img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241212162611052.png" alt="image-20241212162611052" style="zoom:50%;" />

## 0.3 配置环境变量

```bash
export SPARK_HOME=/home/chenmiao/spark
export PATH=$PATH:$SPARK_HOME/bin
export SPARK_CONF_DIR=$SPARK_HOME/conf

source ~/.bashrc	# 激活环境变量
```

## 0.4 配置文件

1. 配置spark-env.sh

   ```sh
   # 将spark-env.sh.template重命名为spark-env.sh
   # 加入以下内容
   export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
   export HADOOP_HOME=/usr/local/hadoop
   export SPARK_MASTER_HOST=localhost
   export SPARK_LOCAL_IP=localhost
   export SPARK_MASTER_PORT=7077
   ```

2. 配置spark-defaults.conf

   ```bash
   # 将spark-defaults.conf.template重命名为spark-defaults.conf
   # 加入以下内容
   spark.master                spark://localhost:7077
   spark.driver.memory         2g
   spark.executor.memory       4g
   spark.eventLog.enabled      true
   ```

3. 配置slaves（需要自己新建）

   ```
   localhost
   ```

## 0.5 启动Spark

**注：**可访问http://localhost:8080检查节点状态

```bash
# 启动Spark Master
$SPARK_HOME/sbin/start-master.sh
# 启动Spark Worker并连接到Master节点
$SPARK_HOME/sbin/start-worker.sh spark://localhost:7077
```

终端运行结果如下：

<img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241212163425513.png" alt="image-20241212163425513" style="zoom: 80%;" />

网页上节点状态显示如下：

<img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241212163555691.png" alt="image-20241212163555691" style="zoom: 33%;" />

```bash
# 停止Spark Worker
$SPARK_HOME/sbin/stop-worker.sh
# 停止Spark Master
$SPARK_HOME/sbin/stop-master.sh
```

## 0.6 遇到的问题

**问题：**后续`spark-submit <python文件名>`运行时，遇到报错，原因是Spark日志事件目录不存在。Spark默认会尝试将事件日志写入`/tmp/spark-events`，但该目录不存在，因此导致`java.io.FileNotFoundException`。

**解决方法：**手动创建日志事件目录

```bash
mkdir -p /tmp/spark-events
chmod 777 /tmp/spark-events
```

# Task1：Spark RDD编程

## 1.1 查询资金流入和流出情况

使用user_balance_table，计算出所有用户在每一天的总资金流入和总资金流出量。

输出格式：<日期>	<资金流入量>	<资金流出量>

### 1.1.1 设计思路

1. 初始化Spark环境

   通过`SparkConf`和`SparkContext`创建Spark运行环境，配置应用名称和运行模式。

   `local[*]`表示在本地运行。

2. 读取文件并过滤表头

3. 解析数据

   提取`report_date`、`total_purchase_amt`和`total_redeem_amt`字段，并将缺失值处理为0。

4. 转换数据格式

5. 聚合数据

   使用`reduceByKey`操作按`report_date`聚合数据，计算总的资金流入和流出。

6. 排序

7. 保存和输出结果

   保存到CSV文件并在终端输出结果。

8. 停止SparkContext

### 1.1.2 运行结果

​	在终端输入`spark-submit task1_1.py`运行代码

* 终端输出结果（节选）

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213162131428.png" alt="image-20241213162131428" style="zoom:50%;" />

* 保存为csv文件截图（节选）

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213162036539.png" alt="image-20241213162036539" style="zoom: 80%;" />

### 1.1.3 程序分析

​	进一步对可能的改进之处进行分析。

1. #### 可扩展性

   不足：代码中字段位置通过硬编码指定，若表结构发生变化，代码需要修改，灵活性较差。

   改进分析：将字段名参数化，避免硬编码。

2. #### 数据加载与解析

   不足：CSV文件的解析是手动完成的，使用`split(",")`。如果数据格式复杂（如字段中包含逗号、引号等特殊字符），可能会导致解析错误。

   改进分析：使用`csv`库进行CSV文件加载和解析。

## 1.2 活跃用户分析

​	使用user_balance_table，定义活跃用户为在指定月份内有至少5天记录的用户，统计2014年8月的活跃用户总数。

### 1.2.1 设计思路

1. 初始化Spark环境

   通过`SparkConf`和`SparkContext`创建Spark运行环境，配置应用名称和运行模式。

   `local[*]`表示在本地运行。

2. 读取文件并过滤表头

3. 解析数据并筛选

   定义目标月份；

   将每行数据按逗号分隔成字段，提取`user_id`和`report_date`；

   筛选出`report_date`中以目标月份开头的记录；

   过滤掉`None`值。

4. 用户活跃天数统计

   使用`map`操作将筛选后的记录转换为`(user_id,report_date)`的格式，然后使用`distinct`操作去除重复的`user_id`，接着使用`map`操作将每个`user_id`映射为1，最后使用`reduceByKey`操作累加每个用户的活跃天数。

5. 筛选并统计活跃用户

   使用`filter`函数，筛选出活跃天数大于等于5的用户；

   对活跃用户RDD调用`count`，统计活跃用户的总数。

6. 输出结果

7. 停止SparkContext

### 1.2.2 运行结果

​	在终端输入`spark-submit task1_2.py`运行代码。

* 终端输出结果

  得到2014年8月的活跃用户总数为：12767

  ![image-20241213170520195](C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213170520195.png)


**注：**因为只有一个数字结果所以就没再写成输出文件了。

### 1.2.3 程序分析

​	进一步对可能的改进之处进行分析。

1. #### 可扩展性

   不足：代码中字段位置通过硬编码指定，若表结构发生变化，代码需要修改，灵活性较差。

   改进分析：将字段名参数化，避免硬编码。

2. #### 数据加载与解析

   不足：CSV文件的解析是手动完成的，使用`split(",")`。如果数据格式复杂（如字段中包含逗号、引号等特殊字符），可能会导致解析错误。

   改进分析：使用`csv`库进行CSV文件加载和解析。

3. #### 性能

   不足：数据集没有进行分区，处理效率较低。

   改进分析：可以对数据进行分区，以提高处理效率。

# Task2：Spark SQL编程

## 2.1 统计特定日期平均余额

按城市统计2014年3月1日的平均余额：计算每个城市在2014年3月1日的用户平均余额(tBalance)，按平均余额降序排列。

输出格式：<城市ID>	<平均余额>

### 2.1.1 设计思路

1. 初始化SparkSession

2. 读取文件并筛选

   使用`spark.read.csv`方法读取两张表`user_profile_table`和`user_balance_table`，筛选`user_balance_table`表中目标日期的数据。

3. 将两个表按user_id进行关联

4. 聚合数据

   按城市分组（`groupBy("City")`），对`tBalance`字段计算平均值（`avg("tBalance")`），然后使用`orderBy`方法对结果进行降序排序。

5. 收集结果到本地

6. 保存和输出结果

   保存到CSV文件并在终端输出结果。

7. 停止SparkContext

### 2.1.2 运行结果

​	在终端输入`spark-submit task2_1.py`运行代码。

* 终端输出结果

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213175717062.png" alt="image-20241213175717062" style="zoom:67%;" />

* 保存为csv文件截图

  ![image-20241213175749060](C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213175749060.png)

### 2.1.3 程序分析

​	进一步对可能的改进之处进行分析。

1. #### 数据存储

   不足：当前结果以CSV格式保存，效率较低。

   改进分析：对于大规模数据，可以使用更高效的存储格式（如Parquet或ORC），以便后续查询和处理。

2. #### CSV字段类型

   不足：没有直接指定CSV每列的字段，而是进行了类型推断，可能带来的性能问题。

   改进分析：在读取CSV文件时，可以显式指定每列的数据类型。

## 2.2 统计每个城市总流量前3高的用户

统计每个城市中每个用户在2014年8月的总流量（定义为total_purchase_amt+total_redeem_amt），并输出每个城市总流量排名前三的用户ID及其总流量。

输出格式：<城市ID>	<用户ID>	<总流量>

### 2.2.1 设计思路

1. 初始化SparkSession

2. 读取数据并创建临时视图
   使用`spark.read.csv`读取CSV文件（`user_profile_table.csv`和`user_balance_table.csv`），并设置`header=True`和`inferSchema=True`，让Spark自动识别列名和数据类型。

   使用`createOrReplaceTempView`将DataFrame注册为Spark SQL临时视图。这使得我们可以直接在SQL查询中操作DataFrame。

3. SQL查询（这部分直接化用了实验三用到的代码）

   ```sql
   # 定义临时结果集CTE user_city_activity
   # 存储2014年8月每个用户的user_id、city、total_traffic（总流量）
   WITH user_city_activity AS (
     SELECT up.user_id, up.city, 
            (SUM(ub.total_purchase_amt) + SUM(ub.total_redeem_amt)) AS total_traffic
     # 通过user_id将两表连接
     FROM user_balance_table ub
     JOIN user_profile_table up ON ub.user_id = up.user_id
     WHERE ub.report_date >= '20140801' AND ub.report_date <= '20140831'	# 筛选日期
     GROUP BY up.user_id, up.city	# 按用户id和城市分组
   ),
   
   # 定义临时结果集CTE ranked_users
   # 使用CTE user_city_activity对每个城市的用户进行排名
   ranked_users AS (
     SELECT user_id, city, total_traffic,
       	 # 按照city分组，总流量降序排列，从1开始分配排名
            ROW_NUMBER() OVER (PARTITION BY city ORDER BY total_traffic DESC) AS rank
     FROM user_city_activity
   )
   
   # 从CTE ranked_users中选择user_id、city和total_traffic
   SELECT CAST(user_id AS BIGINT) AS user_id, 
          city, 
          CAST(total_traffic AS BIGINT) AS total_traffic
   FROM ranked_users
   WHERE rank <= 3		# 筛选每个城市的前三高
   ORDER BY city, rank;
   ```

4. 执行查询并获取结果

   使用`spark.sql`执行SQL查询，并将结果存储在DataFrame中。

5. 收集结果到本地

6. 保存和输出结果

   保存到CSV文件并在终端输出结果。

7. 停止SparkContext

### 2.2.2 运行结果

​	在终端输入`spark-submit task2_2.py`运行代码。

* 终端输出结果

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213184247516.png" alt="image-20241213184247516" style="zoom:80%;" />

* 保存为csv文件截图

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241213184426291.png" alt="image-20241213184426291" style="zoom:80%;" />

### 2.2.3 程序分析

​	进一步对可能的改进之处进行分析。

1. #### 代码健壮性

   不足：没有对异常情况的处理，代码健壮性较弱。

   改进分析：增加异常处理逻辑，确保文件读取、SQL执行、文件写入等过程不会因异常终止。

2. #### 数据预处理

   不足：SQL查询中先加载两张表的完整数据再进行后续操作，计算量稍大。

   改进分析：可以在加载数据时就做一些筛选，减少后续的计算量。例如，`report_date`的筛选条件可以提前在DataFrame层面进行处理。

# Task3：Spark ML编程

使用Spark MLlib提供的机器学习模型，预测2014年9月每天的申购与赎回总额。

## 3.1 设计思路

1. 最开始使用Task1_1得到的20130701~20140831每日资金总流入流出量数据，仅使用日期的“年中第几天”(`day_of_year`)作为特征，使用简单的线性回归做预测，预测结果不理想。
2. 通过浏览平台论坛的帖子，发现2014年1月的数据是存在较大波动的，为了使预测效果更好，采用20140201~20140831七个月的每日资金总流入流出量数据。除了使用“年中第几天”`day_of_year`外，还添加了“哪个月份”`month`和“星期几”`day_of_week`这两个时间特征。为了更好地捕获非线性关系，用随机森林方法替换了线性回归。

## 3.2 运行结果

​	在终端输入`spark-submit task3.py`运行代码

* 保存为csv文件截图

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241216164854149.png" alt="image-20241216164854149" style="zoom: 50%;" />

* 平台评分截图

  <img src="C:\Users\华为\AppData\Roaming\Typora\typora-user-images\image-20241216165615899.png" alt="image-20241216165615899" style="zoom:67%;" />

  **注：**提交文件需要把表头那行删除

## 3.3 程序分析

进一步对可能的改进之处进行分析。

1. #### 数据太过片面

   不足：只使用了过去7个月每天的总资金流入、总资金流出量，并没有结合收益率表和银行间拆借利率表做更加准确的预测。

   改进分析：结合收益率表和银行间拆借利率表中的数据进一步完善模型。

2. #### 随机森林的参数并不是最优的

   不足：没有试着调整随机森林的参数使得预测效果更好

   改进分析：引入交叉验证，设置不同的树的数量（`numTrees`）、树的最大深度（`maxDepth`）参数，找到这些参数的最佳组合。
